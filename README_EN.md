# AgentDoc - æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿ

[English Version](README.md) | ä¸­æ–‡ç‰ˆæœ¬

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## é¡¹ç›®ç®€ä»‹

AgentDoc æ˜¯ä¸€ä¸ªè½»é‡çº§ã€é«˜æ•ˆçš„æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿï¼Œä¸“æ³¨äºæ–‡æ¡£è§£æã€æ™ºèƒ½é—®ç­”å’Œå†…å®¹æ£€ç´¢ã€‚ç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–æ¶æ„è®¾è®¡ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼å¤„ç†ï¼Œæä¾›ç²¾ç¡®çš„å¼•ç”¨ç®¡ç†å’Œæ™ºèƒ½æ¨ç†åŠŸèƒ½ã€‚

## æ ¸å¿ƒç‰¹æ€§

- ğŸ¤– **æ™ºèƒ½é—®ç­”**: åŸºäºæ–‡æ¡£å†…å®¹çš„ç²¾ç¡®é—®ç­”ï¼Œæ”¯æŒä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†
- ğŸ“ **ç²¾ç¡®å¼•ç”¨**: è‡ªåŠ¨æ ‡æ³¨ç­”æ¡ˆæ¥æºï¼Œæä¾›å‡†ç¡®çš„æ–‡æ¡£ä½ç½®å¼•ç”¨
- ğŸš€ **å¤šæ¨¡å‹æ”¯æŒ**: çµæ´»çš„æ¨¡å‹ç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹
- ğŸ“„ **æ–‡æ¡£å¤„ç†**: æ”¯æŒPDFã€DOCXç­‰å¤šç§æ–‡æ¡£æ ¼å¼çš„æ™ºèƒ½è§£æ
- ğŸ”§ **æ¨¡å—åŒ–æ¶æ„**: å¯æ‰©å±•çš„æ’ä»¶ç³»ç»Ÿå’Œå¤„ç†å™¨æ¶æ„
- ğŸ” **æ™ºèƒ½æ£€ç´¢**: é«˜æ•ˆçš„æ–‡æ¡£ç´¢å¼•å’Œæ£€ç´¢ç³»ç»Ÿ
- ğŸ“Š **ä»»åŠ¡é˜Ÿåˆ—**: æ”¯æŒå¼‚æ­¥ä»»åŠ¡å¤„ç†å’Œæ‰¹é‡æ“ä½œ
- âš¡ **è½»é‡é«˜æ•ˆ**: ä¼˜åŒ–çš„æ€§èƒ½å’Œèµ„æºä½¿ç”¨

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- æ¨èä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ

### å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/jweihe/AgentDoc.git
cd AgentDoc

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv agentdoc-env
source agentdoc-env/bin/activate  # Linux/Mac
# æˆ– agentdoc-env\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### åŸºæœ¬ä½¿ç”¨

```python
from agentdoc import ModelManager, PDFProcessor
from agentdoc.qa import QAEngine, DocumentIndexer

# åˆå§‹åŒ–ç»„ä»¶
model_manager = ModelManager()
pdf_processor = PDFProcessor()
qa_engine = QAEngine()

# å¤„ç†æ–‡æ¡£
document = pdf_processor.process("document.pdf")

# å»ºç«‹ç´¢å¼•
indexer = DocumentIndexer()
chunks = indexer.index_document(document)

# æ™ºèƒ½é—®ç­”
result = qa_engine.answer_question(
    question="æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
    chunks=chunks
)

print(f"ç­”æ¡ˆ: {result.answer}")
print(f"å¼•ç”¨: {result.citations}")
```

## é¡¹ç›®ç»“æ„

```
AgentDoc/
â”œâ”€â”€ core/                  # æ ¸å¿ƒæ¨¡å— (é…ç½®ã€æ—¥å¿—ã€å¼‚å¸¸å¤„ç†)
â”œâ”€â”€ models/                # æ¨¡å‹ç®¡ç† (æ¨¡å‹å·¥å‚ã€ç®¡ç†å™¨)
â”œâ”€â”€ processors/            # æ–‡æ¡£å¤„ç†å™¨ (PDFã€æ‰¹é‡å¤„ç†)
â”œâ”€â”€ qa/                    # æ™ºèƒ½é—®ç­” (å¼•æ“ã€ç´¢å¼•ã€æ£€ç´¢ã€æ¨ç†)
â”œâ”€â”€ plugins/               # æ’ä»¶ç³»ç»Ÿ (å¯æ‰©å±•å¤„ç†å™¨)
â”œâ”€â”€ queue/                 # ä»»åŠ¡é˜Ÿåˆ— (å¼‚æ­¥ä»»åŠ¡ç®¡ç†)
â”œâ”€â”€ agents/                # Agentæ¨¡å— (æ™ºèƒ½ä»£ç†)
â”œâ”€â”€ prompts/               # æç¤ºæ¨¡æ¿ (é¢„å®šä¹‰æç¤º)
â”œâ”€â”€ web/                   # Webç•Œé¢
â”œâ”€â”€ api/                   # APIæœåŠ¡
â”œâ”€â”€ cli/                   # å‘½ä»¤è¡Œå·¥å…·
â”œâ”€â”€ tests/                 # æµ‹è¯•ä»£ç 
â”œâ”€â”€ requirements.txt       # é¡¹ç›®ä¾èµ–
â”œâ”€â”€ pyproject.toml         # é¡¹ç›®é…ç½®
â””â”€â”€ README.md              # é¡¹ç›®æ–‡æ¡£
```

## æ ¸å¿ƒæ¨¡å—è¯´æ˜

### ğŸ§  QAæ¨¡å— (agentdoc.qa)
æ™ºèƒ½é—®ç­”ç³»ç»Ÿçš„æ ¸å¿ƒå¼•æ“ï¼Œé‡‡ç”¨RAG (Retrieval-Augmented Generation) æ¶æ„

- **QAEngine**: æ™ºèƒ½é—®ç­”å¼•æ“
  - æ”¯æŒå¤šè½®å¯¹è¯å’Œä¸Šä¸‹æ–‡ç†è§£
  - é›†æˆå‘é‡æ£€ç´¢å’Œè¯­ä¹‰åŒ¹é…
  - æ”¯æŒå¤æ‚æŸ¥è¯¢åˆ†è§£å’Œé‡å†™
  - æä¾›ç½®ä¿¡åº¦è¯„åˆ†å’Œç­”æ¡ˆè´¨é‡è¯„ä¼°

- **DocumentIndexer**: é«˜æ€§èƒ½æ–‡æ¡£ç´¢å¼•å™¨
  - åŸºäºå‘é‡æ•°æ®åº“çš„è¯­ä¹‰ç´¢å¼•
  - æ”¯æŒå¢é‡ç´¢å¼•å’Œå®æ—¶æ›´æ–°
  - å¤šçº§ç´¢å¼•ç­–ç•¥ï¼šç« èŠ‚çº§ã€æ®µè½çº§ã€å¥å­çº§
  - æ™ºèƒ½æ–‡æ¡£åˆ†å—å’Œé‡å å¤„ç†

- **DocumentRetriever**: ç²¾å‡†æ–‡æ¡£æ£€ç´¢å™¨
  - æ··åˆæ£€ç´¢ç­–ç•¥ï¼šå‘é‡æ£€ç´¢ + BM25 + é‡æ’åº
  - æ”¯æŒå¤šæ¨¡æ€æ£€ç´¢ï¼ˆæ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼ï¼‰
  - åŠ¨æ€æ£€ç´¢ç­–ç•¥è°ƒæ•´
  - æ£€ç´¢ç»“æœå»é‡å’Œèšåˆ

- **CitationManager**: æ™ºèƒ½å¼•ç”¨ç®¡ç†å™¨
  - è‡ªåŠ¨ç”Ÿæˆç²¾ç¡®çš„é¡µç å’Œæ®µè½å¼•ç”¨
  - æ”¯æŒå¤šç§å¼•ç”¨æ ¼å¼ï¼ˆAPAã€MLAã€Chicagoç­‰ï¼‰
  - å¼•ç”¨é“¾è¿½è¸ªå’ŒéªŒè¯
  - æ‰¹é‡å¼•ç”¨å¯¼å‡ºåŠŸèƒ½

- **SimpleReasoner**: é€»è¾‘æ¨ç†å¼•æ“
  - æ”¯æŒå› æœæ¨ç†å’Œé€»è¾‘é“¾æ„å»º
  - å¤šæ­¥æ¨ç†å’Œä¸­é—´ç»“æœç¼“å­˜
  - æ¨ç†è·¯å¾„å¯è§†åŒ–å’Œè§£é‡Š
  - æ”¯æŒå‡è®¾éªŒè¯å’Œåé©³è®ºè¯

### ğŸ¤– Modelsæ¨¡å— (models)
ä¼ä¸šçº§æ¨¡å‹ç®¡ç†å’Œè°ƒåº¦ç³»ç»Ÿ

- **ModelManager**: ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨
  - æ”¯æŒå¤šç§LLMï¼šOpenAIã€Claudeã€Qwenã€GLMç­‰
  - æ¨¡å‹è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»
  - å®æ—¶æ€§èƒ½ç›‘æ§å’Œæˆæœ¬ç»Ÿè®¡
  - æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å’ŒA/Bæµ‹è¯•

- **ModelFactory**: æ™ºèƒ½æ¨¡å‹å·¥å‚
  - åŠ¨æ€æ¨¡å‹å®ä¾‹åŒ–å’Œé…ç½®
  - æ¨¡å‹èƒ½åŠ›è‡ªåŠ¨æ£€æµ‹å’ŒåŒ¹é…
  - æ”¯æŒæ¨¡å‹ç»„åˆå’Œçº§è”è°ƒç”¨
  - æ¨¡å‹ç¼“å­˜å’Œé¢„çƒ­æœºåˆ¶

- **BaseModel**: ç»Ÿä¸€æ¨¡å‹æ¥å£
  - æ ‡å‡†åŒ–APIæ¥å£è®¾è®¡
  - æ”¯æŒæµå¼è¾“å‡ºå’Œæ‰¹é‡å¤„ç†
  - å†…ç½®é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
  - æ¨¡å‹è°ƒç”¨é“¾è·¯è¿½è¸ª

### ğŸ”Œ Pluginsæ¨¡å— (plugins)
å¯æ‰©å±•çš„æ’ä»¶ç”Ÿæ€ç³»ç»Ÿ

- **PluginManager**: æ’ä»¶ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨
  - çƒ­æ’æ‹”æ’ä»¶åŠ è½½å’Œå¸è½½
  - æ’ä»¶ä¾èµ–ç®¡ç†å’Œç‰ˆæœ¬æ§åˆ¶
  - æ’ä»¶å®‰å…¨æ²™ç®±å’Œæƒé™æ§åˆ¶
  - æ’ä»¶æ€§èƒ½ç›‘æ§å’Œèµ„æºé™åˆ¶

- **BasePlugin**: æ’ä»¶å¼€å‘æ¡†æ¶
  - æ ‡å‡†åŒ–æ’ä»¶æ¥å£å’Œç”Ÿå‘½å‘¨æœŸ
  - æ’ä»¶é…ç½®ç®¡ç†å’Œå‚æ•°éªŒè¯
  - æ’ä»¶é—´é€šä¿¡å’Œäº‹ä»¶æœºåˆ¶
  - æ’ä»¶é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

- **ProcessorPlugin**: æ–‡æ¡£å¤„ç†æ’ä»¶
  - æ”¯æŒè‡ªå®šä¹‰æ–‡æ¡£æ ¼å¼è§£æ
  - æ–‡æ¡£é¢„å¤„ç†å’Œåå¤„ç†ç®¡é“
  - å¤šè¯­è¨€æ–‡æ¡£å¤„ç†æ”¯æŒ
  - æ–‡æ¡£è´¨é‡è¯„ä¼°å’Œä¼˜åŒ–

### âš¡ Queueæ¨¡å— (queue)
é«˜æ€§èƒ½å¼‚æ­¥ä»»åŠ¡å¤„ç†ç³»ç»Ÿ

- **TaskQueue**: åˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—
  - æ”¯æŒRedisã€RabbitMQç­‰æ¶ˆæ¯é˜Ÿåˆ—
  - ä»»åŠ¡ä¼˜å…ˆçº§å’Œå»¶è¿Ÿæ‰§è¡Œ
  - ä»»åŠ¡å»é‡å’Œå¹‚ç­‰æ€§ä¿è¯
  - æ­»ä¿¡é˜Ÿåˆ—å’Œä»»åŠ¡é‡è¯•æœºåˆ¶

- **TaskManager**: ä»»åŠ¡è°ƒåº¦ç®¡ç†å™¨
  - ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸå…¨ç¨‹è·Ÿè¸ª
  - åŠ¨æ€å·¥ä½œè¿›ç¨‹æ‰©ç¼©å®¹
  - ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡å’Œæ€§èƒ½åˆ†æ
  - ä»»åŠ¡ä¾èµ–ç®¡ç†å’Œæ‰¹é‡æ“ä½œ

- **Worker**: é«˜æ•ˆä»»åŠ¡æ‰§è¡Œå™¨
  - å¤šè¿›ç¨‹/å¤šçº¿ç¨‹å¹¶å‘æ‰§è¡Œ
  - ä»»åŠ¡æ‰§è¡Œç¯å¢ƒéš”ç¦»
  - èµ„æºä½¿ç”¨ç›‘æ§å’Œé™åˆ¶
  - ä»»åŠ¡æ‰§è¡Œæ—¥å¿—å’Œé”™è¯¯è¿½è¸ª

### ğŸ“„ Processorsæ¨¡å— (processors)
å¤šæ ¼å¼æ–‡æ¡£å¤„ç†å¼•æ“

- **PDFProcessor**: PDFæ–‡æ¡£ä¸“ä¸šå¤„ç†å™¨
  - é«˜ç²¾åº¦æ–‡æœ¬æå–å’Œç‰ˆé¢åˆ†æ
  - è¡¨æ ¼ã€å›¾ç‰‡ã€å…¬å¼è¯†åˆ«
  - OCRé›†æˆå’Œæ–‡å­—è¯†åˆ«
  - æ–‡æ¡£ç»“æ„åŒ–å’Œå…ƒæ•°æ®æå–

- **BatchProcessor**: æ‰¹é‡å¤„ç†è°ƒåº¦å™¨
  - å¤§è§„æ¨¡æ–‡æ¡£å¹¶è¡Œå¤„ç†
  - å¤„ç†è¿›åº¦å®æ—¶ç›‘æ§
  - é”™è¯¯æ¢å¤å’Œæ–­ç‚¹ç»­ä¼ 
  - å¤„ç†ç»“æœç»Ÿè®¡å’ŒæŠ¥å‘Š

### ğŸ¯ Agentsæ¨¡å— (agents)
æ™ºèƒ½ä»£ç†å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿ

- **DocumentAgent**: æ–‡æ¡£æ™ºèƒ½ä»£ç†
  - è‡ªåŠ¨æ–‡æ¡£åˆ†æå’Œæ‘˜è¦ç”Ÿæˆ
  - æ–‡æ¡£è´¨é‡è¯„ä¼°å’Œæ”¹è¿›å»ºè®®
  - å¤šæ–‡æ¡£å…³è”åˆ†æå’Œå¯¹æ¯”
  - æ–‡æ¡£çŸ¥è¯†å›¾è°±æ„å»º

### ğŸŒ Webæ¨¡å— (web)
ç°ä»£åŒ–Webç•Œé¢

- **WebUI**: å“åº”å¼ç”¨æˆ·ç•Œé¢
  - æ‹–æ‹½å¼æ–‡æ¡£ä¸Šä¼ å’Œç®¡ç†
  - å®æ—¶é—®ç­”å’Œç»“æœå±•ç¤º
  - å¯è§†åŒ–åˆ†æå’Œå›¾è¡¨å±•ç¤º
  - å¤šç”¨æˆ·åä½œå’Œæƒé™ç®¡ç†

### ğŸ”§ Utilsæ¨¡å— (utils)
é€šç”¨å·¥å…·å’Œè¾…åŠ©åŠŸèƒ½

- **ConfigManager**: é…ç½®ç®¡ç†å™¨
- **Logger**: ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿ
- **FileHandler**: æ–‡ä»¶æ“ä½œå·¥å…·
- **TextProcessor**: æ–‡æœ¬å¤„ç†å·¥å…·

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æ’ä»¶å¼€å‘

```python
from plugins import BasePlugin, PluginManager

class CustomProcessor(BasePlugin):
    def process(self, document):
        # è‡ªå®šä¹‰å¤„ç†é€»è¾‘
        return processed_document

# æ³¨å†Œæ’ä»¶
plugin_manager = PluginManager()
plugin_manager.register_plugin("custom", CustomProcessor())
```

### æ‰¹é‡æ–‡æ¡£å¤„ç†

```python
from processors import BatchProcessor
from task_queue import TaskQueue

# æ‰¹é‡å¤„ç†
batch_processor = BatchProcessor()
task_queue = TaskQueue()

# æ·»åŠ ä»»åŠ¡
for pdf_file in pdf_files:
    task_queue.add_task("process_pdf", {"file_path": pdf_file})

# æ‰§è¡Œæ‰¹é‡å¤„ç†
results = batch_processor.process_batch(task_queue)
```

### é…ç½®ç®¡ç†

```python
from core import Settings

# è‡ªå®šä¹‰é…ç½®
settings = Settings(
    model_name="qwen2.5",
    max_chunk_size=1000,
    overlap_size=100,
    citation_enabled=True
)
```

## å¼€å‘æŒ‡å—

### ç¯å¢ƒè®¾ç½®

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install pytest black flake8

# ä»£ç æ ¼å¼åŒ–
black .

# ä»£ç æ£€æŸ¥
flake8 .
```

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# è¿è¡Œç‰¹å®šæ¨¡å—æµ‹è¯•
python test_qa_module.py
python test_plugins.py
```

### ğŸ¤– Model Download and Setup

AgentDoc supports Qwen2.5 and Qwen3 series models for local inference. Follow these steps to download and test models:

#### Download Models

```bash
# List all available models
python3 download_models.py list

# Download a specific model (recommended: start with smaller models)
python3 download_models.py download qwen2.5-0.5b-instruct

# Download with ModelScope mirror for faster speed in China
python3 download_models.py download qwen2.5-7b-instruct --modelscope

# Check download status
python3 download_models.py status
```

#### Available Models

**Qwen2.5 Series:**
| Model | Size | Description | Recommended Use |
|-------|------|-------------|----------------|
| qwen2.5-0.5b-instruct | ~1GB | Lightweight version | Testing, development |
| qwen2.5-1.5b-instruct | ~3GB | Balanced performance | Small-scale production |
| qwen2.5-3b-instruct | ~6GB | Medium scale | General purpose |
| qwen2.5-7b-instruct | ~15GB | High performance | Production workloads |
| qwen2.5-14b-instruct | ~30GB | Large scale | Advanced tasks |
| qwen2.5-32b-instruct | ~65GB | Very large scale | Complex reasoning |
| qwen2.5-72b-instruct | ~145GB | Flagship model | Maximum performance |

**Qwen3 Series (Latest Generation):**
| Model | Size | Description | Recommended Use |
|-------|------|-------------|----------------|
| qwen3-0.6b | ~1.2GB | Next-gen ultra-lightweight | Testing, development |
| qwen3-1.7b | ~3.5GB | Next-gen lightweight | Small-scale production |
| qwen3-4b | ~8GB | Next-gen medium scale | General purpose |
| qwen3-8b | ~16GB | Next-gen high performance | Production workloads |
| qwen3-14b | ~28GB | Next-gen large scale | Advanced tasks |
| qwen3-32b | ~64GB | Next-gen very large scale | Complex reasoning |

#### Test Downloaded Models

```bash
# List downloaded models
python3 test_model.py list

# Test a specific model
python3 test_model.py test qwen2.5-0.5b-instruct

# Test with custom prompt
python3 test_model.py test qwen2.5-0.5b-instruct --prompt "Hello, please introduce yourself"
```

#### Using Local Qwen Models

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model and tokenizer
model_path = "models/downloads/qwen2.5-0.5b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Generate response
prompt = "Hello, how can I help you today?"
inputs = tokenizer(prompt, return_tensors="pt")
with torch.no_grad():
    outputs = model.generate(
        inputs.input_ids,
        max_length=512,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```